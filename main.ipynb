{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "videoFilePath = \"\"\n",
    "saveDirPath = \"\"\n",
    "audioFilePath = saveDirPath + \"\"\n",
    "srtFilePath = saveDirPath + \"\"\n",
    "languages = \"\"\n",
    "use_denoising = True\n",
    "\n",
    "model_name = \"openai/whisper-large-v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract audio from video\n",
    "import subprocess\n",
    "\n",
    "command = (\n",
    "    'ffmpeg -y -i \"{}\" -ab 160k -ac 2 -ar 44100 -vn \"{}\"'\n",
    "    .format(videoFilePath, audioFilePath)\n",
    ")\n",
    "\n",
    "return_code = subprocess.call(command, shell=True)\n",
    "\n",
    "if return_code == 0:\n",
    "    print(f\"Successfully converted audio, please check \\\"{audioFilePath}\\\"\")\n",
    "else:\n",
    "    print(f\"Error: ffmpeg command failed with return code {return_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from denoiser import pretrained\n",
    "from denoiser.dsp import convert_audio\n",
    "\n",
    "def reduce_noise(audio, denoiser_model, device='cuda'):\n",
    "    audio = audio.to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(device_type=device, dtype=torch.float16):\n",
    "        denoised_audio = denoiser_model(audio)\n",
    "\n",
    "    denoised_audio = denoised_audio.squeeze()\n",
    "        \n",
    "    return denoised_audio\n",
    "\n",
    "def load_audio(file_path):\n",
    "    speech_array, sampling_rate = torchaudio.load(file_path)\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "        speech_array = resampler(speech_array)\n",
    "    if speech_array.shape[0] > 1:\n",
    "        speech_array = torch.mean(speech_array, dim=0, keepdim=False)\n",
    "    return speech_array\n",
    "\n",
    "if use_denoising:\n",
    "    denoiser_model = pretrained.dns64().cuda()\n",
    "    speech = load_audio(audioFilePath)\n",
    "    speech = speech.unsqueeze(0)\n",
    "    speech = reduce_noise(speech, denoiser_model)\n",
    "\n",
    "    del denoiser_model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CUDA\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# load whisper & processor\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init pipeline\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=1,\n",
    "    device=device,\n",
    "    return_timestamps=True,\n",
    "    generate_kwargs={\n",
    "        \"language\": languages,\n",
    "        \"task\": \"transcribe\",\n",
    "        \"num_beams\": 10,  # default 5\n",
    "        #\"length_penalty\": 1.0,\n",
    "        #\"early_stopping\": True,\n",
    "        #\"no_repeat_ngram_size\": 3,  \n",
    "        #\"repetition_penalty\": 1.2  \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if str(type(speech)) != \"<class 'numpy.ndarray'>\":\n",
    "    speech = speech.cpu().numpy()\n",
    "\n",
    "result = asr_pipeline(speech)\n",
    "transcript = result.get('chunks', [])\n",
    "\n",
    "del asr_pipeline\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display audio\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def play_audio(audio, sr=16000):\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio_np = audio.cpu().squeeze().numpy()\n",
    "    elif isinstance(audio, np.ndarray):\n",
    "        audio_np = audio\n",
    "    else:\n",
    "        raise TypeError(\"Audio data must be in torch.Tensor or numpy.ndarray format.\")\n",
    "    \n",
    "    display(Audio(audio_np, rate=sr))\n",
    "\n",
    "\n",
    "play_audio(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_srt(transcript, srt_path):\n",
    "    with open(srt_path, 'w', encoding='utf-8') as srt_file:\n",
    "        for idx, segment in enumerate(transcript, start=1):\n",
    "            # Verify that the 'timestamp' key exists and the value is valid\n",
    "            if 'timestamp' not in segment or segment['timestamp'] is None:\n",
    "                print(f\"Warning: Segment {idx} missing 'timestamp' key or 'timestamp' is None. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            start_time, end_time = segment['timestamp']\n",
    "\n",
    "            # Verify that the start and end times are not None\n",
    "            if start_time is None or end_time is None:\n",
    "                print(f\"Warning: Segment {idx} has None start or end time. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            text = segment['text'].strip()\n",
    "\n",
    "            def format_timestamp(seconds):\n",
    "                if seconds is None:\n",
    "                    return \"00:00:00,000\"\n",
    "                millis = int((seconds - int(seconds)) * 1000)\n",
    "                hrs = int(seconds // 3600)\n",
    "                mins = int((seconds % 3600) // 60)\n",
    "                secs = int(seconds % 60)\n",
    "                return f\"{hrs:02}:{mins:02}:{secs:02},{millis:03}\"\n",
    "\n",
    "            srt_file.write(f\"{idx}\\n\")\n",
    "            srt_file.write(f\"{format_timestamp(start_time)} --> {format_timestamp(end_time)}\\n\")\n",
    "            srt_file.write(f\"{text}\\n\\n\")\n",
    "\n",
    "\n",
    "# create .srt file\n",
    "create_srt(transcript, srtFilePath)\n",
    "\n",
    "print(f\"Subtitle file created at \\\"{srtFilePath}\\\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
